apiVersion: v1
data:
  get_metrics.py: "from __future__ import print_function\nfrom kubernetes import client\nfrom
    kubernetes.client.rest import ApiException\nfrom pprint import pprint\nfrom influxdb
    import InfluxDBClient\nfrom influxdb.client import InfluxDBClientError\nfrom datetime
    import datetime\nfrom time import sleep\nimport time\nimport sys\nimport ast\nimport
    os\n\nwith open(\"/run/secrets/kubernetes.io/serviceaccount/token\", \"r\") as
    file:\n    api_token = file.read().strip('\\n')\n\ncluster_name = os.environ.get('CLUSTER_NAME',
    'demo_cluster')\napi_endpoint = os.environ.get('API_ENDPOINT', 'api.sno.openshift.training')\ninfluxdb_endpoint
    = os.environ.get('INFLUXDB_ENDPOINT', 'influxdb.apps.sno.openshift.training')\n
    \nconfiguration = client.Configuration()\nconfiguration.api_key['authorization']
    = api_token\nconfiguration.verify_ssl = True\nconfiguration.api_key_prefix['authorization']
    = 'Bearer'\nconfiguration.host = \"https://\" + api_endpoint + \":6443\"\n\ndef
    get_resourcequota():\n    with client.ApiClient(configuration) as api_client:\n
    \       api_instance = client.CoreV1Api(api_client)\n        # _continue = '_continue_example'\n
    \       pretty = 'true'\n        \n        try:\n            api_response = api_instance.list_resource_quota_for_all_namespaces(pretty=pretty)\n
    \           # pprint(api_response)\n        except ApiException as e:\n            print(\"Exception
    when calling CoreV1Api->list_resource_quota_for_all_namespaces: %s\\n\" % e)\n\n
    \   # for v in api_response.items:\n    #     print(v.metadata.namespace,\n    #
    \        v.status.hard['limits.cpu'], \n    #         v.status.hard['limits.memory'],
    \n    #         v.status.used['limits.cpu'], \n    #         v.status.used['limits.memory'])\n
    \   return api_response\n\n    \ndef get_namespace(label):\n    with client.ApiClient(configuration)
    as api_client:\n        api_instance = client.CoreV1Api(api_client)\n        #
    _continue = '_continue_example'\n        pretty = 'true'\n        label_selector
    = 'label'\n        \n        try:\n            api_response = api_instance.list_namespace(pretty=pretty,
    label_selector=label_selector)\n            # pprint(api_response)\n        except
    ApiException as e:\n            print(\"Exception when calling CoreV1Api->list_namespace:
    %s\\n\" % e)\n\n    for ns in api_response.items:\n        print(ns.metadata.labels['uaid'],
    ns.metadata.name)\n\ndef get_nodestats(node):\n    with client.ApiClient(configuration)
    as api_client:\n        # Create an instance of the API class\n        api_instance
    = client.CoreV1Api(api_client)\n        name = node # str | name of the NodeProxyOptions\n
    \       path = '/stats/summary' # str | path to the resource\n\n        try:\n
    \           api_response = api_instance.connect_get_node_proxy_with_path(name,
    path)\n            # pprint(api_response)\n            return api_response\n        except
    ApiException as e:\n            print(\"Exception when calling CoreV1Api->connect_get_node_proxy_with_path:
    %s\\n\" % e)\n\n\ndef get_node_list():\n    with client.ApiClient(configuration)
    as api_client:\n        # Create an instance of the API class\n        api_instance
    = client.CoreV1Api(api_client)\n        pretty = 'true' # str | If 'true', then
    the output is pretty printed. Defaults to 'false' unless the user-agent indicates
    a browser or command-line HTTP tool (curl and wget). (optional)\n        # allow_watch_bookmarks
    = True # bool | allowWatchBookmarks requests watch events with type \\\"BOOKMARK\\\".
    Servers that do not implement bookmarks may ignore this flag and bookmarks are
    sent at the server's discretion. Clients should not assume bookmarks are returned
    at any specific interval, nor may they assume the server will send any BOOKMARK
    event during a session. If this is not a watch, this field is ignored. (optional)\n
    \       # _continue = '_continue_example' # str | The continue option should be
    set when retrieving more results from the server. Since this value is server defined,
    kubernetes.clients may only use the continue value from a previous query result
    with identical query parameters (except for the value of continue) and the server
    may reject a continue value it does not recognize. If the specified continue value
    is no longer valid whether due to expiration (generally five to fifteen minutes)
    or a configuration change on the server, the server will respond with a 410 ResourceExpired
    error together with a continue token. If the kubernetes.client needs a consistent
    list, it must restart their list without the continue field. Otherwise, the kubernetes.client
    may send another list request with the token received with the 410 error, the
    server will respond with a list starting from the next key, but from the latest
    snapshot, which is inconsistent from the previous list results - objects that
    are created, modified, or deleted after the first list request will be included
    in the response, as long as their keys are after the \\\"next key\\\".  This field
    is not supported when watch is true. Clients may start a watch from the last resourceVersion
    value returned by the server and not miss any modifications. (optional)\n        #
    field_selector = 'field_selector_example' # str | A selector to restrict the list
    of returned objects by their fields. Defaults to everything. (optional)\n        #
    label_selector = 'label_selector_example' # str | A selector to restrict the list
    of returned objects by their labels. Defaults to everything. (optional)\n        #
    limit = 56 # int | limit is a maximum number of responses to return for a list
    call. If more items exist, the server will set the `continue` field on the list
    metadata to a value that can be used with the same initial query to retrieve the
    next set of results. Setting a limit may return fewer than the requested amount
    of items (up to zero items) in the event all requested objects are filtered out
    and kubernetes.clients should only use the presence of the continue field to determine
    whether more results are available. Servers may choose not to support the limit
    argument and will return all of the available results. If limit is specified and
    the continue field is empty, kubernetes.clients may assume that no more results
    are available. This field is not supported if watch is true.  The server guarantees
    that the objects returned when using continue will be identical to issuing a single
    list call without a limit - that is, no objects created, modified, or deleted
    after the first request is issued will be included in any subsequent continued
    requests. This is sometimes referred to as a consistent snapshot, and ensures
    that a kubernetes.client that is using limit to receive smaller chunks of a very
    large result can ensure they see all possible objects. If objects are updated
    during a chunked list the version of the object that was present at the time the
    first list result was calculated is returned. (optional)\n        # resource_version
    = 'resource_version_example' # str | resourceVersion sets a constraint on what
    resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions
    for details.  Defaults to unset (optional)\n        # resource_version_match =
    'resource_version_match_example' # str | resourceVersionMatch determines how resourceVersion
    is applied to list calls. It is highly recommended that resourceVersionMatch be
    set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions
    for details.  Defaults to unset (optional)\n        # send_initial_events = True
    # bool | `sendInitialEvents=true` may be set together with `watch=true`. In that
    case, the watch stream will begin with synthetic events to produce the current
    state of objects in the collection. Once all such events have been sent, a synthetic
    \\\"Bookmark\\\" event  will be sent. The bookmark will report the ResourceVersion
    (RV) corresponding to the set of objects, and be marked with `\\\"k8s.io/initial-events-end\\\":
    \\\"true\\\"` annotation. Afterwards, the watch stream will proceed as usual,
    sending watch events corresponding to changes (subsequent to the RV) to objects
    watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch`
    option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch`
    = NotOlderThan   is interpreted as \\\"data at least as new as the provided `resourceVersion`\\\"
    \  and the bookmark event is send when the state is synced   to a `resourceVersion`
    at least as fresh as the one provided by the ListOptions.   If `resourceVersion`
    is unset, this is interpreted as \\\"consistent read\\\" and the   bookmark event
    is send when the state is synced at least to the moment   when request started
    being processed. - `resourceVersionMatch` set to any other value or unset   Invalid
    error is returned.  Defaults to true if `resourceVersion=\\\"\\\"` or `resourceVersion=\\\"0\\\"`
    (for backward compatibility reasons) and to false otherwise. (optional)\n        #
    timeout_seconds = 56 # int | Timeout for the list/watch call. This limits the
    duration of the call, regardless of any activity or inactivity. (optional)\n        #
    watch = True # bool | Watch for changes to the described resources and return
    them as a stream of add, update, and remove notifications. Specify resourceVersion.
    (optional)\n\n        try:\n            api_response = api_instance.list_node(pretty=pretty)\n
    \           # pprint(api_response)\n            return api_response\n        except
    ApiException as e:\n            print(\"Exception when calling CoreV1Api->list_node:
    %s\\n\" % e)\n\ndef send_data_influx(data):\n    # Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImluZmx1eGRiIiwiZXhwIjoxNzY5NDcwNzYzLjU1NzI4Mn0.qU9vt_gQB4SMn1ywDYv-fPR58I53afq3kZwi27aA9hI\n
    \   series = []\n    now = datetime.today()\n    influxdb_token='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImluZmx1eGRiIiwiZXhwIjoxNzY5NDcwNzYzLjU1NzI4Mn0.qU9vt_gQB4SMn1ywDYv-fPR58I53afq3kZwi27aA9hI'\n
    \   client = InfluxDBClient(host=influxdb_endpoint, \n                            port=443,
    ssl=True, verify_ssl=True, \n                            username=None, password=None,
    \n                            headers={\"Authorization\": \"Bearer \" + influxdb_token},\n
    \                           database='openshift'\n                            )\n
    \   version = client.ping()\n    print(\"Successfully connected to InfluxDB: \"
    + version)\n\n    # node_start_time = int(datetime.strptime(data['node']['startTime'],
    \"%Y-%m-%dT%H:%M:%SZ\").strftime('%s'))\n    node_start_time = data['node']['startTime']\n
    \   node_name = data['node']['nodeName']\n\n    for nk, nv in data['node'].items():\n
    \       if isinstance(nv, dict) and nk not in ['systemContainers', 'network']:\n
    \           if nk == 'runtime':\n                for rtk, rtv in nv.items():\n
    \                   if isinstance(rtv, dict):\n                        field_time
    = rtv['time']\n                        pointValues = {\n                                \"time\":
    field_time,\n                                \"measurement\": 'node_' + nk + '_'
    + rtk,\n                                \"fields\": rtv,\n                                \"tags\":
    {\n                                    \"node_name\": node_name,\n                                    \"cluster_name\":
    cluster_name,\n                                    \"runtime\": 1,\n                                },\n
    \                           }\n\n                        pointValues['fields'].pop('time')\n
    \                       pointValues['fields'].update({'node_start_time': node_start_time})\n
    \                       series.append(pointValues)\n            else:\n                pointValues
    = {\n                        \"time\": nv['time'],\n                        \"measurement\":
    'node_' + nk,\n                        \"fields\": nv,\n                        \"tags\":
    {\n                            \"node_name\": node_name,\n                            \"cluster_name\":
    cluster_name, \n                        },\n                    }\n\n                pointValues['fields'].pop('time')\n
    \               pointValues['fields'].update({'node_start_time': node_start_time})\n
    \               series.append(pointValues)\n        \n        # pprint(series)\n\n
    \   for sc in data['node']['systemContainers']:\n        for k,v in sc.items():\n
    \           if isinstance(v, dict):\n                field_time = v['time']\n
    \               pointValues = {\n                        \"time\": field_time,\n
    \                       \"measurement\": 'container_' + k,\n                        \"fields\":
    v,\n                        \"tags\": {\n                            \"node_name\":
    node_name,\n                            \"cluster_name\": cluster_name, \n                            \"container_name\":
    sc['name'], \n                            \"container_type\": 'systemContainers',
    \n                        },\n                    }\n        \n                pointValues['fields'].pop('time')\n
    \               pointValues['fields'].update({'node_start_time': node_start_time})\n
    \               pointValues['fields'].update({'container_start_time': sc['startTime']})\n
    \               series.append(pointValues)\n                # pprint(series)\n\n
    \   for pod in data['pods']:\n        for k,v in pod.items():\n            if
    isinstance(v, dict) and k not in ['podRef', 'containers', 'volume', 'process_stats']:\n
    \               field_time = v['time']\n                pointValues = {\n                        \"time\":
    field_time,\n                        \"measurement\": 'pod_' + k,\n                        \"fields\":
    v,\n                        \"tags\": {\n                            \"node_name\":
    node_name,\n                            \"cluster_name\": cluster_name, \n                            \"container_type\":
    'namespace', \n                        },\n                    }\n        \n                t
    = pointValues['fields'].pop('time')\n                pointValues['fields'].update({'node_start_time':
    node_start_time})\n                pointValues['fields'].update({'pod_start_time':
    pod['startTime']})\n                pointValues['tags'].update(pod['podRef'])\n
    \               # pprint(series)\n\n                if k == 'cpu':\n                    pointValues['fields'].update({'process_count':
    pod['process_stats']['process_count']})\n\n                if k == 'network':\n
    \                   pointValues['tags'].update({\"network_name\": v['name']})\n
    \                   ii = pointValues['fields'].pop('interfaces')\n                    series.append(pointValues)\n
    \                   for i in ii:\n                        pointValues = {\n                                \"time\":
    t,\n                                \"measurement\": 'pod_' + k + '_interface',\n
    \                               \"fields\": i,\n                                \"tags\":
    {\n                                    \"node_name\": node_name,\n                                    \"cluster_name\":
    cluster_name, \n                                    \"network_name\": v['name'],
    \n                                    \"interface_name\": i['name'], \n                                    \"container_type\":
    'namespace', \n                                },\n                            }\n
    \               \n                        pointValues['fields'].pop('name')\n
    \                       pointValues['tags'].update(pod['podRef'])\n                        series.append(pointValues)\n
    \                       # pprint(pointValues)\n\n                else: \n                    series.append(pointValues)\n\n\n
    \           if k == 'volume' and isinstance(v, list):\n                for vv
    in v:\n                    field_time = vv['time']\n                    pointValues
    = {\n                            \"time\": field_time,\n                            \"measurement\":
    'pod_' + k,\n                            \"fields\": vv,\n                            \"tags\":
    {\n                                \"node_name\": node_name,\n                                \"cluster_name\":
    cluster_name, \n                                \"volume_name\": vv['name'], \n
    \                               \"container_type\": 'namespace', \n                            },\n
    \                       }\n            \n                    pointValues['fields'].pop('name')\n
    \                   pointValues['fields'].pop('time')\n                    if
    'pvcRef' in pointValues['fields']:\n                        pvc = pointValues['fields'].pop('pvcRef')\n
    \                       pointValues['tags'].update({'pvc_ref_name': pvc['name']})\n
    \                       pointValues['tags'].update({'pvc_ref_namespace': pvc['namespace']})\n
    \                   pointValues['fields'].update({'node_start_time': node_start_time})\n
    \                   pointValues['fields'].update({'pod_start_time': pod['startTime']})\n
    \                   pointValues['tags'].update(pod['podRef'])\n                    series.append(pointValues)\n
    \                   # pprint(pointValues)\n\n\n            if isinstance(v, list)
    and k =='containers':\n                for c in v:\n                    for k,m
    in c.items():\n                        if isinstance(m, dict):\n                            field_time
    = c['startTime']\n                            pointValues = {\n                                    \"time\":
    field_time,\n                                    \"measurement\": 'contanier_'
    + k ,\n                                    \"fields\": m,\n                                    \"tags\":
    {\n                                        \"node_name\": node_name,\n                                        \"cluster_name\":
    cluster_name, \n                                        \"container_type\": 'namespace',
    \n                                        \"container_name\": c['name'], \n                                    },\n
    \                               }\n                    \n                            pointValues['fields'].pop('time')\n
    \                           pointValues['fields'].update({'node_start_time': node_start_time})\n
    \                           pointValues['fields'].update({'pod_start_time': pod['startTime']})\n
    \                           pointValues['fields'].update({'container_start_time':
    field_time})\n                            pointValues['tags'].update({'pod_name':
    pod['podRef']['name']})\n                            pointValues['tags'].update({'pod_namespace':
    pod['podRef']['namespace']})\n                            pointValues['tags'].update({'pod_uid':
    pod['podRef']['uid']})\n                            series.append(pointValues)\n
    \                           # pprint(pointValues)\n\n\n    # pprint(series)\n
    \   # retention_policy = 'server_data'\n    # client.create_retention_policy(retention_policy,
    '30d', 3, default=True)\n    client.write_points(series)\n\nif __name__ == '__main__':\n
    \   for node in get_node_list().items:\n        send_data_influx(ast.literal_eval(get_nodestats(node.metadata.name)))\n
    \   \n    # send_data_influx()\n    sys.stdout.flush()\n    time.sleep(1)"
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: python-script-get-metrics
